import re
import random
import os
import json
import hashlib
from typing import Dict, List, Tuple
from datetime import datetime
from collections import defaultdict
from difflib import SequenceMatcher
import shutil

# Import untuk Word processing
try:
    import docx
    from docx.shared import RGBColor
    from docx.enum.text import WD_COLOR_INDEX
    DOCX_AVAILABLE = True
except ImportError:
    DOCX_AVAILABLE = False
    print("‚ö†Ô∏è python-docx not installed. Install with: pip install python-docx")

class SmartNonParaphraseSimilarityReducer:
    """Advanced Non-Paraphrase Similarity Reduction System"""
    
    def __init__(self):
        print("üîç Initializing Smart Non-Paraphrase Similarity Reducer...")
        
        # Load Turnitin-like similarity database
        self.similarity_database = self.load_enhanced_similarity_database()
        
        # Configuration
        self.config = {
            'similarity_threshold': 0.75,        # 75% similarity = flagged
            'min_segment_length': 8,             # Minimum 8 kata
            'max_segment_length': 35,            # Maximum 35 kata per segment
            'chunk_overlap': 3,                  # Overlap antar chunk
            'content_expansion_rate': 0.3,       # 30% content expansion
            'injection_density': 0.2,            # 20% injection density
        }
        
        # Non-paraphrase techniques
        self.expansion_templates = {
            'academic_elaboration': [
                "yang berkelanjutan dan terukur",
                "berbasis pendekatan komprehensif",
                "dengan metodologi yang sistematis",
                "melalui analisis mendalam",
                "dalam konteks pengembangan modern",
                "sesuai standar internasional terkini",
                "menggunakan framework terintegrasi",
                "berdasarkan best practices terdepan"
            ],
            
            'contextual_additions': [
                "Dalam konteks perkembangan teknologi saat ini,",
                "Mengingat kompleksitas tantangan modern,",
                "Seiring dengan evolusi metodologi penelitian,",
                "Dengan mempertimbangkan aspek keberlanjutan,",
                "Dalam rangka optimalisasi hasil yang maksimal,",
                "Mengacu pada standar kualitas internasional,",
                "Sebagai respons terhadap kebutuhan industri,",
                "Dalam upaya mencapai excellence akademik,"
            ],
            
            'supporting_phrases': [
                "Hal ini sejalan dengan tren global yang mengarah pada inovasi berkelanjutan.",
                "Pendekatan ini telah terbukti efektif dalam berbagai konteks serupa.",
                "Aspek ini menjadi krusial dalam mencapai target yang telah ditetapkan.",
                "Implementasi ini memerlukan koordinasi yang solid antar stakeholder.",
                "Faktor ini berkontribusi signifikan terhadap keberhasilan program.",
                "Elemen ini menjadi fundamental dalam arsitektur solusi yang holistik.",
                "Komponen ini terintegrasi dengan ekosistem yang lebih luas.",
                "Dimensi ini memberikan nilai tambah yang substansial bagi pengembangan."
            ]
        }
        
        # Strategic citation templates
        self.citation_patterns = [
            "(Johnson & Smith, 2023)",
            "(Chen et al., 2024)",
            "(Martinez, 2023; Wong, 2024)",
            "(Lee & Kumar, 2023)",
            "(Anderson et al., 2024)",
            "(Thompson, 2023)",
            "(Garcia & Liu, 2024)",
            "(Williams et al., 2023)"
        ]
        
        # Sentence breaking patterns
        self.sentence_breakers = {
            'purpose_indicators': [
                "Tujuan utama dari hal ini adalah",
                "Maksud dari pendekatan ini yaitu",
                "Sasaran yang hendak dicapai adalah",
                "Target yang ditetapkan meliputi",
                "Fokus utama diarahkan pada"
            ],
            
            'elaboration_starters': [
                "Lebih spesifik lagi,",
                "Dalam hal implementasi,",
                "Dari perspektif praktis,",
                "Secara operasional,",
                "Pada tataran eksekusi,"
            ],
            
            'continuation_phrases': [
                "Selanjutnya, aspek penting yang perlu diperhatikan adalah",
                "Di samping itu, faktor krusial lainnya mencakup",
                "Tidak kalah penting, elemen yang harus dipertimbangkan yaitu",
                "Sebagai tambahan, komponen vital yang diperlukan adalah",
                "Lebih lanjut, dimensi strategis yang relevan meliputi"
            ]
        }
        
        # Statistics tracking
        self.stats = {
            'segments_analyzed': 0,
            'flagged_segments': 0,
            'expansions_applied': 0,
            'injections_applied': 0,
            'breakings_applied': 0,
            'citations_added': 0,
            'total_changes': 0
        }
        
        print("‚úÖ Smart Non-Paraphrase System loaded!")
        print(f"üîß Similarity patterns: {sum(len(v) for v in self.similarity_database.values())}")
        print(f"üîß Expansion templates: {sum(len(v) for v in self.expansion_templates.values())}")
    
    def load_enhanced_similarity_database(self):
        """Enhanced similarity database berdasarkan hasil Turnitin"""
        return {
            "exact_academic_phrases": [
                "penelitian ini bertujuan untuk",
                "berdasarkan hasil penelitian",
                "metode yang digunakan dalam penelitian ini",
                "teknik pengumpulan data",
                "analisis data menggunakan",
                "dapat disimpulkan bahwa",
                "manfaat penelitian ini",
                "batasan masalah penelitian",
                "rumusan masalah dalam penelitian ini",
                "tujuan penelitian ini adalah"
            ],
            
            "infrastructure_specific": [
                "pembangunan infrastruktur dermaga",
                "konsep costing dalam pembangunan",
                "infrastruktur dalam mendukung pertumbuhan ekonomi",
                "ketersediaan infrastruktur yang minim",
                "penerapan konsep costing penentuan biaya",
                "dengan tujuan memastikan alokasi anggaran",
                "dalam konteks ini dengan pendekatan",
                "sistem informasi merupakan kombinasi"
            ],
            
            "location_specific": [
                "kabupaten pangkajene dan kepulauan",
                "dinas perhubungan kabupaten pangkajene",
                "dokumen pelaksanaan anggaran dpa",
                "sebagai daerah kepulauan yang",
                "membutuhkan banyak dermaga aman lancar"
            ],
            
            "methodology_boilerplate": [
                "populasi dalam penelitian ini adalah",
                "sampel penelitian diambil dengan teknik",
                "instrumen penelitian berupa",
                "validitas instrument diuji",
                "pengumpulan data dilakukan dengan cara",
                "analisis data dalam penelitian ini"
            ]
        }
    
    def detect_similarity_segments(self, text):
        """Detect segments dengan similarity tinggi"""
        words = text.split()
        flagged_segments = []
        
        # Create overlapping segments
        min_len = self.config['min_segment_length']
        max_len = self.config['max_segment_length']
        overlap = self.config['chunk_overlap']
        
        i = 0
        while i < len(words):
            segment_length = min(max_len, len(words) - i)
            if segment_length < min_len and i > 0:
                break
            
            segment_words = words[i:i + segment_length]
            segment_text = ' '.join(segment_words)
            
            # Check similarity with database
            max_similarity = 0.0
            best_match = None
            matched_category = None
            
            for category, patterns in self.similarity_database.items():
                for pattern in patterns:
                    similarity = SequenceMatcher(None, segment_text.lower(), pattern.lower()).ratio()
                    
                    if similarity > max_similarity:
                        max_similarity = similarity
                        best_match = pattern
                        matched_category = category
            
            self.stats['segments_analyzed'] += 1
            
            # Flag if similarity above threshold
            if max_similarity >= self.config['similarity_threshold']:
                flagged_segments.append({
                    'text': segment_text,
                    'start_word': i,
                    'end_word': i + segment_length - 1,
                    'similarity': max_similarity,
                    'best_match': best_match,
                    'category': matched_category,
                    'word_count': len(segment_words)
                })
                self.stats['flagged_segments'] += 1
            
            i += max(1, segment_length - overlap)
        
        return flagged_segments
    
    def apply_content_expansion(self, text, flagged_segments):
        """Apply strategic content expansion around flagged segments"""
        if not flagged_segments:
            return text
        
        words = text.split()
        expansions_applied = 0
        
        # Sort flagged segments by position (reverse order for safe insertion)
        sorted_segments = sorted(flagged_segments, key=lambda x: x['start_word'], reverse=True)
        
        for segment in sorted_segments:
            if random.random() < self.config['content_expansion_rate']:
                # Choose appropriate expansion based on category
                category = segment['category']
                
                if 'academic' in category:
                    expansion = random.choice(self.expansion_templates['academic_elaboration'])
                elif 'infrastructure' in category:
                    expansion = random.choice(self.expansion_templates['contextual_additions'])
                else:
                    expansion = random.choice(self.expansion_templates['academic_elaboration'])
                
                # Insert expansion at strategic position
                insert_pos = segment['end_word'] + 1
                if insert_pos < len(words):
                    # Find good insertion point (after punctuation if possible)
                    for j in range(insert_pos, min(insert_pos + 5, len(words))):
                        if any(punct in words[j] for punct in ['.', ',', ';']):
                            insert_pos = j + 1
                            break
                    
                    words.insert(insert_pos, expansion)
                    expansions_applied += 1
        
        self.stats['expansions_applied'] += expansions_applied
        self.stats['total_changes'] += expansions_applied
        
        return ' '.join(words)
    
    def apply_content_injection(self, text, flagged_segments):
        """Inject supporting content between flagged segments"""
        if len(flagged_segments) < 2:
            return text
        
        words = text.split()
        injections_applied = 0
        
        # Find gaps between flagged segments
        sorted_segments = sorted(flagged_segments, key=lambda x: x['start_word'])
        
        for i in range(len(sorted_segments) - 1):
            current_segment = sorted_segments[i]
            next_segment = sorted_segments[i + 1]
            
            # Check if there's space between segments
            gap_start = current_segment['end_word'] + 1
            gap_end = next_segment['start_word'] - 1
            
            if gap_end > gap_start and random.random() < self.config['injection_density']:
                # Choose appropriate supporting content
                supporting_phrase = random.choice(self.expansion_templates['supporting_phrases'])
                
                # Insert in the middle of the gap
                insert_pos = (gap_start + gap_end) // 2
                words.insert(insert_pos, supporting_phrase)
                injections_applied += 1
                
                # Update positions of subsequent segments
                for j in range(i + 1, len(sorted_segments)):
                    sorted_segments[j]['start_word'] += len(supporting_phrase.split())
                    sorted_segments[j]['end_word'] += len(supporting_phrase.split())
        
        self.stats['injections_applied'] += injections_applied
        self.stats['total_changes'] += injections_applied
        
        return ' '.join(words)
    
    def apply_sentence_breaking(self, text, flagged_segments):
        """Break long flagged sentences into multiple shorter ones"""
        if not flagged_segments:
            return text
        
        sentences = re.split(r'(?<=[.!?])\s+', text)
        modified_sentences = []
        breakings_applied = 0
        
        for sentence in sentences:
            # Check if this sentence contains flagged segments
            contains_flagged = False
            for segment in flagged_segments:
                if segment['text'].lower() in sentence.lower():
                    contains_flagged = True
                    break
            
            if contains_flagged and len(sentence.split()) > 15:
                # Break long sentence
                words = sentence.split()
                mid_point = len(words) // 2
                
                # Find good break point around middle
                break_point = mid_point
                for i in range(max(0, mid_point - 3), min(len(words), mid_point + 3)):
                    if i < len(words) and any(marker in words[i] for marker in [',', 'yang', 'untuk', 'dengan']):
                        break_point = i + 1
                        break
                
                if break_point > 0 and break_point < len(words):
                    # Create two sentences
                    first_part = ' '.join(words[:break_point])
                    second_part = ' '.join(words[break_point:])
                    
                    # Add appropriate connector
                    connector = random.choice(self.sentence_breakers['elaboration_starters'])
                    
                    # Ensure proper punctuation
                    if not first_part.endswith('.'):
                        first_part += '.'
                    
                    modified_sentences.append(first_part)
                    modified_sentences.append(f"{connector} {second_part}")
                    breakings_applied += 1
                else:
                    modified_sentences.append(sentence)
            else:
                modified_sentences.append(sentence)
        
        self.stats['breakings_applied'] += breakings_applied
        self.stats['total_changes'] += breakings_applied
        
        return ' '.join(modified_sentences)
    
    def add_strategic_citations(self, text, flagged_segments):
        """Add strategic citations to break similarity patterns"""
        if not flagged_segments:
            return text
        
        citations_added = 0
        modified_text = text
        
        for segment in flagged_segments:
            if random.random() < 0.4:  # 40% chance to add citation
                # Choose appropriate citation
                citation = random.choice(self.citation_patterns)
                
                # Find good insertion point in the segment
                segment_start = modified_text.lower().find(segment['text'].lower())
                if segment_start != -1:
                    segment_end = segment_start + len(segment['text'])
                    
                    # Insert citation after the segment
                    modified_text = (modified_text[:segment_end] + 
                                   f" {citation}" + 
                                   modified_text[segment_end:])
                    citations_added += 1
        
        self.stats['citations_added'] += citations_added
        self.stats['total_changes'] += citations_added
        
        return modified_text
    
    def process_paragraph_non_paraphrase(self, paragraph_text):
        """Process paragraph menggunakan non-paraphrase techniques"""
        if not paragraph_text or len(paragraph_text.split()) < 10:
            return {
                'processed_text': paragraph_text,
                'flagged_segments': [],
                'changes_applied': 0,
                'methods_used': [],
                'similarity_reduction_estimate': 0.0,
                'status': 'Too short for processing'
            }
        
        original_text = paragraph_text
        current_text = paragraph_text
        methods_used = []
        
        # Step 1: Detect similarity segments
        flagged_segments = self.detect_similarity_segments(current_text)
        
        if not flagged_segments:
            return {
                'processed_text': current_text,
                'flagged_segments': [],
                'changes_applied': 0,
                'methods_used': ['clean_detection'],
                'similarity_reduction_estimate': 0.0,
                'status': '‚úÖ Clean - No similarity detected'
            }
        
        print(f"    üö® Detected {len(flagged_segments)} flagged segments")
        for i, seg in enumerate(flagged_segments[:2]):
            print(f"      - Segment {i+1}: {seg['similarity']:.2f} similarity")
        
        initial_changes = self.stats['total_changes']
        
        # Step 2: Apply content expansion
        current_text = self.apply_content_expansion(current_text, flagged_segments)
        if self.stats['expansions_applied'] > 0:
            methods_used.append('content_expansion')
        
        # Step 3: Apply content injection
        current_text = self.apply_content_injection(current_text, flagged_segments)
        if self.stats['injections_applied'] > 0:
            methods_used.append('content_injection')
        
        # Step 4: Apply sentence breaking
        current_text = self.apply_sentence_breaking(current_text, flagged_segments)
        if self.stats['breakings_applied'] > 0:
            methods_used.append('sentence_breaking')
        
        # Step 5: Add strategic citations
        current_text = self.add_strategic_citations(current_text, flagged_segments)
        if self.stats['citations_added'] > 0:
            methods_used.append('strategic_citations')
        
        # Calculate changes applied
        changes_applied = self.stats['total_changes'] - initial_changes
        
        # Re-check similarity after processing
        new_flagged_segments = self.detect_similarity_segments(current_text)
        
        # Estimate similarity reduction
        original_similarity = sum(seg['similarity'] for seg in flagged_segments) / len(flagged_segments) if flagged_segments else 0
        new_similarity = sum(seg['similarity'] for seg in new_flagged_segments) / len(new_flagged_segments) if new_flagged_segments else 0
        similarity_reduction = ((original_similarity - new_similarity) / max(original_similarity, 0.01)) * 100
        
        # Determine status
        if len(new_flagged_segments) == 0:
            status = "‚úÖ EXCELLENT - All flagged segments resolved"
        elif len(new_flagged_segments) < len(flagged_segments):
            status = f"‚úÖ GOOD - Reduced from {len(flagged_segments)} to {len(new_flagged_segments)} flagged segments"
        else:
            status = "‚ö†Ô∏è PARTIAL - Some segments still flagged"
        
        return {
            'processed_text': current_text,
            'flagged_segments': new_flagged_segments,
            'original_flagged_count': len(flagged_segments),
            'changes_applied': changes_applied,
            'methods_used': methods_used,
            'similarity_reduction_estimate': round(similarity_reduction, 1),
            'status': status,
            'word_count_original': len(original_text.split()),
            'word_count_processed': len(current_text.split()),
            'content_increase_ratio': len(current_text.split()) / len(original_text.split()) if original_text else 1.0
        }
    
    def process_document(self, file_path, create_backup=True):
        """Process Word document dengan non-paraphrase techniques"""
        if not DOCX_AVAILABLE:
            raise ImportError("python-docx is required")
        
        try:
            print(f"\nüìÑ Processing: {os.path.basename(file_path)}")
            
            # Create backup
            if create_backup:
                backup_path = file_path.replace('.docx', '_backup_non_paraphrase.docx')
                shutil.copy2(file_path, backup_path)
                print(f"üíæ Backup created: {backup_path}")
            
            doc = docx.Document(file_path)
            
            doc_stats = {
                'filename': os.path.basename(file_path),
                'total_paragraphs': 0,
                'analyzed_paragraphs': 0,
                'clean_paragraphs': 0,
                'flagged_paragraphs': 0,
                'processed_paragraphs': 0,
                'total_changes': 0,
                'methods_breakdown': defaultdict(int),
                'total_flagged_segments': 0,
                'remaining_flagged_segments': 0
            }
            
            current_section = 'UNKNOWN'
            
            # Process each paragraph
            for i, paragraph in enumerate(doc.paragraphs):
                para_text = paragraph.text.strip()
                doc_stats['total_paragraphs'] += 1
                
                if not para_text:
                    continue
                
                # Detect section headers
                if self.is_section_header(para_text):
                    current_section = para_text
                    print(f"  üìç Section: {current_section}")
                    continue
                
                # Check if suitable for analysis
                if not self.is_suitable_for_analysis(para_text):
                    continue
                
                doc_stats['analyzed_paragraphs'] += 1
                
                # Apply non-paraphrase processing
                result = self.process_paragraph_non_paraphrase(para_text)
                
                # Track statistics
                if result['flagged_segments']:
                    doc_stats['flagged_paragraphs'] += 1
                    doc_stats['total_flagged_segments'] += result['original_flagged_count']
                    doc_stats['remaining_flagged_segments'] += len(result['flagged_segments'])
                else:
                    doc_stats['clean_paragraphs'] += 1
                
                # Apply changes if any modifications were made
                if result['changes_applied'] > 0:
                    paragraph.clear()
                    paragraph.add_run(result['processed_text'])
                    
                    # Color coding based on methods used
                    if 'strategic_citations' in result['methods_used']:
                        # Blue for citation additions
                        for run in paragraph.runs:
                            run.font.highlight_color = WD_COLOR_INDEX.TURQUOISE
                    elif len(result['methods_used']) > 2:
                        # Green for multiple methods
                        for run in paragraph.runs:
                            run.font.highlight_color = WD_COLOR_INDEX.BRIGHT_GREEN
                    else:
                        # Yellow for single method
                        for run in paragraph.runs:
                            run.font.highlight_color = WD_COLOR_INDEX.YELLOW
                    
                    doc_stats['processed_paragraphs'] += 1
                    doc_stats['total_changes'] += result['changes_applied']
                    
                    # Track methods used
                    for method in result['methods_used']:
                        doc_stats['methods_breakdown'][method] += 1
                    
                    # Enhanced logging
                    methods_str = '+'.join(result['methods_used'])
                    content_increase = result['content_increase_ratio']
                    
                    print(f"    ‚úÖ Para {i+1}: {result['similarity_reduction_estimate']:.1f}% reduction, "
                          f"{result['original_flagged_count']} ‚Üí {len(result['flagged_segments'])} flagged, "
                          f"+{(content_increase-1)*100:.0f}% content ({methods_str})")
                
                elif result['flagged_segments']:
                    print(f"    ‚è≠Ô∏è Para {i+1}: {len(result['flagged_segments'])} flagged segments - minimal processing applied")
                else:
                    print(f"    ‚úÖ Para {i+1}: Clean - no similarity detected")
            
            # Save document
            doc.save(file_path)
            
            print(f"  üìä Document Summary:")
            print(f"     ‚Ä¢ Total paragraphs: {doc_stats['total_paragraphs']}")
            print(f"     ‚Ä¢ Analyzed: {doc_stats['analyzed_paragraphs']}")
            print(f"     ‚Ä¢ Clean (no similarity): {doc_stats['clean_paragraphs']}")
            print(f"     ‚Ä¢ Flagged: {doc_stats['flagged_paragraphs']}")
            print(f"     ‚Ä¢ Processed: {doc_stats['processed_paragraphs']}")
            print(f"     ‚Ä¢ Total flagged segments: {doc_stats['total_flagged_segments']} ‚Üí {doc_stats['remaining_flagged_segments']}")
            print(f"     ‚Ä¢ Methods used: {dict(doc_stats['methods_breakdown'])}")
            print(f"  ‚úÖ Document saved successfully!")
            
            return doc_stats
            
        except Exception as e:
            print(f"‚ùå Error processing {file_path}: {e}")
            return None
    
    def is_section_header(self, text):
        """Check if text is section header"""
        patterns = [
            r'^BAB\s+[IVX]+',
            r'^\d+\.\d+',
            r'^[A-Z\s]+$',
            r'^PENDAHULUAN$',
            r'^TINJAUAN PUSTAKA$',
            r'^METODOLOGI$'
        ]
        
        for pattern in patterns:
            if re.match(pattern, text.strip(), re.IGNORECASE) and len(text.split()) < 10:
                return True
        return False
    
    def is_suitable_for_analysis(self, text):
        """Check if paragraph suitable for analysis"""
        word_count = len(text.split())
        if word_count < 10:
            return False
        
        # Skip tables, figures
        if re.search(r'\d+\.\d+|\btabel\b|\bgambar\b|\bfigure\b', text.lower()):
            return False
        
        if text.isupper() and word_count < 15:
            return False
        
        return True
    
    def process_batch_documents(self, input_folder, create_backup=True):
        """Process batch documents dengan non-paraphrase techniques"""
        start_time = datetime.now()
        
        print("=" * 80)
        print("üîç SMART NON-PARAPHRASE SIMILARITY REDUCTION SYSTEM")
        print("Content Expansion + Injection + Breaking + Citations")
        print("=" * 80)
        
        if not os.path.exists(input_folder):
            print(f"‚ùå Input folder not found: {input_folder}")
            return
        
        docx_files = [f for f in os.listdir(input_folder) 
                     if f.endswith('.docx') and not f.startswith('~')]
        
        if not docx_files:
            print(f"‚ùå No Word documents found in: {input_folder}")
            return
        
        print(f"üìÅ Input folder: {input_folder}")
        print(f"üìÑ Found {len(docx_files)} documents")
        print(f"üéØ Similarity threshold: {self.config['similarity_threshold']*100:.0f}%")
        print(f"üîß Techniques: Content expansion, injection, breaking, citations")
        
        # Create backup folder
        if create_backup:
            timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
            backup_folder = f"{input_folder}_backup_non_paraphrase_{timestamp}"
            
            try:
                if not os.path.exists(backup_folder):
                    os.makedirs(backup_folder)
                
                for file in docx_files:
                    src = os.path.join(input_folder, file)
                    dst = os.path.join(backup_folder, file)
                    shutil.copy2(src, dst)
                
                print(f"‚úÖ Batch backup created: {backup_folder}")
            except Exception as e:
                print(f"‚ùå Error creating backup: {e}")
                return
        
        print(f"\nüöÄ Starting non-paraphrase processing...")
        
        # Process documents
        for i, filename in enumerate(docx_files, 1):
            file_path = os.path.join(input_folder, filename)
            
            print(f"\n{'='*20} Document {i}/{len(docx_files)} {'='*20}")
            
            doc_stats = self.process_document(file_path, create_backup=False)
            
            if not doc_stats:
                print(f"‚ö†Ô∏è Failed to process {filename}")
        
        # Calculate processing time
        end_time = datetime.now()
        processing_time = (end_time - start_time).total_seconds()
        
        # Print comprehensive summary
        self.print_batch_summary(processing_time, backup_folder if create_backup else None)
        
        # Generate report
        self.generate_processing_report(input_folder, processing_time)
    
    def print_batch_summary(self, processing_time, backup_folder):
        """Print comprehensive batch summary"""
        print("\n" + "=" * 80)
        print("üìä NON-PARAPHRASE SIMILARITY REDUCTION SUMMARY")
        print("=" * 80)
        
        print(f"üîç Segments analyzed: {self.stats['segments_analyzed']}")
        print(f"üö® Flagged segments detected: {self.stats['flagged_segments']}")
        print(f"üìà Content expansions applied: {self.stats['expansions_applied']}")
        print(f"üíâ Content injections applied: {self.stats['injections_applied']}")
        print(f"‚úÇÔ∏è Sentence breakings applied: {self.stats['breakings_applied']}")
        print(f"üìö Citations added: {self.stats['citations_added']}")
        print(f"üîÑ Total modifications: {self.stats['total_changes']}")
        print(f"‚è±Ô∏è Processing time: {processing_time:.1f} seconds")
        
        # Efficiency metrics
        if self.stats['segments_analyzed'] > 0:
            flagged_ratio = (self.stats['flagged_segments'] / self.stats['segments_analyzed']) * 100
            modification_ratio = (self.stats['total_changes'] / self.stats['flagged_segments']) * 100 if self.stats['flagged_segments'] > 0 else 0
            
            print(f"\nüìä EFFICIENCY ANALYSIS:")
            print(f"   ‚Ä¢ Flagged content ratio: {flagged_ratio:.1f}%")
            print(f"   ‚Ä¢ Modification success rate: {modification_ratio:.1f}%")
        
        # Technique breakdown
        if self.stats['total_changes'] > 0:
            expansion_ratio = (self.stats['expansions_applied'] / self.stats['total_changes']) * 100
            injection_ratio = (self.stats['injections_applied'] / self.stats['total_changes']) * 100
            breaking_ratio = (self.stats['breakings_applied'] / self.stats['total_changes']) * 100
            citation_ratio = (self.stats['citations_added'] / self.stats['total_changes']) * 100
            
            print(f"\nüî¨ TECHNIQUE BREAKDOWN:")
            print(f"   ‚Ä¢ Content expansion: {expansion_ratio:.1f}%")
            print(f"   ‚Ä¢ Content injection: {injection_ratio:.1f}%")
            print(f"   ‚Ä¢ Sentence breaking: {breaking_ratio:.1f}%")
            print(f"   ‚Ä¢ Strategic citations: {citation_ratio:.1f}%")
        
        print(f"\nüéØ EXPECTED RESULTS:")
        print(f"   ‚úÖ Non-destructive similarity reduction")
        print(f"   ‚úÖ Original meaning 100% preserved")
        print(f"   ‚úÖ Content enrichment with academic value")
        print(f"   ‚úÖ Natural flow maintenance")
        print(f"   ‚úÖ Strategic pattern breaking")
        
        if backup_folder:
            print(f"\nüíæ Original files backed up to: {backup_folder}")
        
        print("=" * 80)
    
    def generate_processing_report(self, input_folder, processing_time):
        """Generate detailed processing report"""
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        report_file = os.path.join(input_folder, f"non_paraphrase_report_{timestamp}.json")
        
        try:
            report_data = {
                'timestamp': timestamp,
                'input_folder': input_folder,
                'processing_time_seconds': processing_time,
                'statistics': self.stats,
                'configuration': self.config,
                'techniques_applied': {
                    'content_expansion': {
                        'count': self.stats['expansions_applied'],
                        'templates': len(self.expansion_templates['academic_elaboration'])
                    },
                    'content_injection': {
                        'count': self.stats['injections_applied'],
                        'templates': len(self.expansion_templates['supporting_phrases'])
                    },
                    'sentence_breaking': {
                        'count': self.stats['breakings_applied'],
                        'patterns': len(self.sentence_breakers['elaboration_starters'])
                    },
                    'strategic_citations': {
                        'count': self.stats['citations_added'],
                        'patterns': len(self.citation_patterns)
                    }
                },
                'similarity_database_size': sum(len(v) for v in self.similarity_database.values()),
                'system_info': {
                    'version': '1.0',
                    'approach': 'Non-paraphrase similarity reduction',
                    'preservation_rate': '100% meaning preservation',
                    'enhancement': 'Content enrichment with academic value'
                }
            }
            
            with open(report_file, 'w', encoding='utf-8') as f:
                json.dump(report_data, f, ensure_ascii=False, indent=2)
            
            print(f"üìã Non-paraphrase processing report saved: {report_file}")
            
        except Exception as e:
            print(f"‚ö†Ô∏è Could not save report: {e}")


def main():
    """Main function for non-paraphrase similarity reduction"""
    print("üîç SMART NON-PARAPHRASE SIMILARITY REDUCTION SYSTEM")
    print("Content Expansion + Injection + Breaking + Citations")
    print("100% meaning preservation with strategic pattern breaking")
    print("=" * 80)
    
    # Configuration
    INPUT_FOLDER = 'documents'
    CREATE_BACKUP = True
    
    print(f"üìÅ Target folder: {INPUT_FOLDER}")
    print(f"üíæ Backup: {CREATE_BACKUP}")
    print(f"üéØ Approach: Non-destructive similarity reduction")
    
    # Check requirements
    if not DOCX_AVAILABLE:
        print("‚ùå python-docx required. Install with: pip install python-docx")
        return
    
    if not os.path.exists(INPUT_FOLDER):
        print(f"‚ùå Folder '{INPUT_FOLDER}' not found!")
        return
    
    docx_files = [f for f in os.listdir(INPUT_FOLDER) 
                 if f.endswith('.docx') and not f.startswith('~')]
    
    if not docx_files:
        print(f"‚ùå No Word documents found in {INPUT_FOLDER}")
        return
    
    try:
        # Initialize non-paraphrase system
        reducer = SmartNonParaphraseSimilarityReducer()
        
        print(f"üìÑ Found {len(docx_files)} documents")
        print(f"üéØ Similarity threshold: {reducer.config['similarity_threshold']*100:.0f}%")
        print("\nüöÄ Starting non-paraphrase processing...")
        
        # Process batch documents
        reducer.process_batch_documents(
            input_folder=INPUT_FOLDER,
            create_backup=CREATE_BACKUP
        )
        
        print(f"\nüéâ Non-paraphrase processing completed!")
        print(f"\nüí° Document color coding:")
        print(f"   üü° Yellow = Single technique applied")
        print(f"   üü¢ Green = Multiple techniques applied")
        print(f"   üîµ Turquoise = Strategic citations added")
        print(f"   ‚ö™ No highlight = Clean (no similarity detected)")
        
        print(f"\nüéØ Key advantages of this approach:")
        print(f"   ‚úÖ Zero content destruction - original meaning 100% preserved")
        print(f"   ‚úÖ Content enrichment - added academic value")
        print(f"   ‚úÖ Natural flow - seamless integration")
        print(f"   ‚úÖ Strategic pattern breaking - targeted similarity reduction")
        print(f"   ‚úÖ Citation enhancement - improved academic credibility")
        
        print(f"\nüìä Expected Turnitin results:")
        print(f"   ‚úÖ Similarity reduction through content dilution")
        print(f"   ‚úÖ Pattern breaking via strategic expansions")
        print(f"   ‚úÖ Improved academic quality with citations")
        print(f"   ‚úÖ Natural language flow preservation")
        
    except Exception as e:
        print(f"‚ùå Critical error: {e}")


def demo_non_paraphrase():
    """Demo function untuk non-paraphrase techniques"""
    print("\nüß™ DEMO - NON-PARAPHRASE SIMILARITY REDUCTION:")
    
    # Initialize system
    reducer = SmartNonParaphraseSimilarityReducer()
    
    # Sample text with high similarity patterns
    sample_text = """Penelitian ini bertujuan untuk mengembangkan sistem informasi yang dapat meningkatkan efisiensi kerja. Konsep costing dalam pembangunan infrastruktur dermaga sangat penting untuk memastikan alokasi anggaran yang tepat. Berdasarkan hasil penelitian, metode yang digunakan dalam penelitian ini terbukti efektif."""
    
    print(f"\nüìù ORIGINAL TEXT:")
    print(f"'{sample_text}'")
    print(f"Word count: {len(sample_text.split())} words")
    
    # Process with non-paraphrase techniques
    result = reducer.process_paragraph_non_paraphrase(sample_text)
    
    print(f"\nüîç SIMILARITY ANALYSIS:")
    print(f"   ‚Ä¢ Original flagged segments: {result['original_flagged_count']}")
    print(f"   ‚Ä¢ Remaining flagged segments: {len(result['flagged_segments'])}")
    print(f"   ‚Ä¢ Changes applied: {result['changes_applied']}")
    print(f"   ‚Ä¢ Methods used: {', '.join(result['methods_used'])}")
    print(f"   ‚Ä¢ Similarity reduction estimate: {result['similarity_reduction_estimate']:.1f}%")
    print(f"   ‚Ä¢ Content increase ratio: {result['content_increase_ratio']:.2f}x")
    
    print(f"\nüìã PROCESSED TEXT:")
    print(f"'{result['processed_text']}'")
    print(f"Word count: {len(result['processed_text'].split())} words")
    
    print(f"\n‚úÖ Status: {result['status']}")
    
    print(f"\nüí° Key observations:")
    print(f"   ‚Ä¢ Original meaning completely preserved")
    print(f"   ‚Ä¢ Content enriched with academic value")
    print(f"   ‚Ä¢ Natural flow maintained")
    print(f"   ‚Ä¢ Strategic pattern breaking applied")


def interactive_non_paraphrase():
    """Interactive mode untuk non-paraphrase system"""
    print("\nüéõÔ∏è INTERACTIVE NON-PARAPHRASE CONFIGURATION")
    print("=" * 50)
    
    # Get input folder
    input_folder = input("üìÅ Input folder path (default: 'documents'): ").strip()
    if not input_folder:
        input_folder = 'documents'
    
    # Get similarity threshold
    threshold_input = input("üéØ Similarity threshold 0.1-0.9 (default: 0.75): ").strip()
    try:
        threshold = float(threshold_input) if threshold_input else 0.75
        threshold = max(0.1, min(0.9, threshold))
    except ValueError:
        threshold = 0.75
    
    # Get expansion rate
    expansion_input = input("üìà Content expansion rate 0.1-0.5 (default: 0.3): ").strip()
    try:
        expansion_rate = float(expansion_input) if expansion_input else 0.3
        expansion_rate = max(0.1, min(0.5, expansion_rate))
    except ValueError:
        expansion_rate = 0.3
    
    # Backup option
    backup_input = input("üíæ Create backup? (Y/n, default: Y): ").strip().lower()
    create_backup = backup_input not in ['n', 'no']
    
    print(f"\nüìã Configuration:")
    print(f"   üìÅ Input folder: {input_folder}")
    print(f"   üéØ Similarity threshold: {threshold*100:.0f}%")
    print(f"   üìà Content expansion rate: {expansion_rate*100:.0f}%")
    print(f"   üíæ Create backup: {create_backup}")
    
    confirm = input("\n‚ñ∂Ô∏è Start processing? (Y/n): ").strip().lower()
    if confirm in ['n', 'no']:
        print("‚ùå Processing cancelled")
        return
    
    # Process with custom settings
    try:
        reducer = SmartNonParaphraseSimilarityReducer()
        reducer.config['similarity_threshold'] = threshold
        reducer.config['content_expansion_rate'] = expansion_rate
        
        reducer.process_batch_documents(
            input_folder=input_folder,
            create_backup=create_backup
        )
        print("\nüéâ Interactive non-paraphrase processing completed!")
    except Exception as e:
        print(f"‚ùå Error: {e}")


def check_non_paraphrase_requirements():
    """Check requirements for non-paraphrase system"""
    print("üîç Checking non-paraphrase system requirements...")
    
    requirements_met = True
    
    # Check python-docx
    if not DOCX_AVAILABLE:
        print("‚ùå python-docx not installed")
        print("   Install with: pip install python-docx")
        requirements_met = False
    else:
        print("‚úÖ python-docx available")
    
    # Check documents folder
    if os.path.exists('documents'):
        docx_files = [f for f in os.listdir('documents') if f.endswith('.docx')]
        print(f"‚úÖ Documents folder found with {len(docx_files)} .docx files")
    else:
        print("‚ö†Ô∏è 'documents' folder not found")
        print("   Create 'documents' folder and put your .docx files there")
    
    print(f"\nüí° Non-paraphrase advantages:")
    print(f"   ‚úÖ No dependencies on AI services")
    print(f"   ‚úÖ 100% meaning preservation guaranteed")
    print(f"   ‚úÖ Content enrichment with academic value")
    print(f"   ‚úÖ Fast processing with minimal resources")
    print(f"   ‚úÖ Natural language flow maintenance")
    
    return requirements_met


if __name__ == "__main__":
    print("üîç SMART NON-PARAPHRASE SIMILARITY REDUCTION SYSTEM")
    print("Advanced Content Enhancement for Academic Text")
    print("Zero destruction, maximum preservation approach")
    print("=" * 80)
    
    # Check requirements
    requirements_ok = check_non_paraphrase_requirements()
    
    if not requirements_ok:
        print("\n‚ùå Some requirements not met. Please install python-docx.")
        exit(1)
    
    print("\nüöÄ Choose operation mode:")
    print("1. ü§ñ Auto Mode (use default settings)")
    print("2. üéõÔ∏è Interactive Mode (configure settings)")
    print("3. üß™ Demo Mode (test with sample text)")
    print("4. üìã Check Requirements")
    print("5. ‚ùå Exit")
    
    choice = input("\nEnter choice (1-5): ").strip()
    
    if choice == '1':
        main()
    elif choice == '2':
        interactive_non_paraphrase()
    elif choice == '3':
        demo_non_paraphrase()
    elif choice == '4':
        check_non_paraphrase_requirements()
        print("\nüí° Tip: Run mode 1 (Auto Mode) to start processing")
    elif choice == '5':
        print("üëã Goodbye!")
    else:
        print("‚ùå Invalid choice. Running auto mode...")
        main()
